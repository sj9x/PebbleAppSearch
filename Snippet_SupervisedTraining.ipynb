{
 "metadata": {
  "name": "",
  "signature": "sha256:d40f558e3d310c7ee3c03be9f39b703844cfdb76b1cd6626bc165acd1d6a2d9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' This file contains the functions used to categorize the data for the Pebble apps based on the app description \n",
      "and app preset category\n",
      "The general procedure is Natural language processing: Translation, Tokenization, Stemming\n",
      "Feature vector creation using Bag of words or TFIDF\n",
      "Feature reduction \n",
      "Training and creation of ensemble of models\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pandas import read_csv, DataFrame\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "import random\n",
      "import re\n",
      "import pickle\n",
      "from numpy import unique\n",
      "from detectlanguage import detect_language\n",
      "import goslate\n",
      "\n",
      "from nltk.tokenize import RegexpTokenizer\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
      "from nltk.stem.porter import *\n",
      "\n",
      "from sklearn.metrics import accuracy_score,precision_score,classification_report\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.decomposition import PCA\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.utils import resample\n",
      "from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Import Data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data=read_csv(\"/Users/Jackass/Insight project/rows.csv\")\n",
      "data_translated=read_csv(\"dataLabeled.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "preset_unique=unique(data.ix[:,'category'])\n",
      "f= lambda x: x.lower()\n",
      "np.array(A)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Prepocessing Module"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''This module contains the preprocessing functions for the description\n",
      "    The functions in this class returns the list of features of words'''\n",
      "def stem_data(text):\n",
      "    stemmer = PorterStemmer()\n",
      "    tokenizer=RegexpTokenizer(r'[a-zA-Z]+')\n",
      "    data=[]\n",
      "    for lines in text:\n",
      "        tokenized_text=tokenizer.tokenize(lines.lower()) \n",
      "        for i in range(len(tokenized_text)):\n",
      "            tokenized_text[i]=str(stemmer.stem(tokenized_text[i]))\n",
      "        data.append(\" \".join(tokenized_text))\n",
      "    return data\n",
      "\n",
      "def words_len3(text):\n",
      "\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words''' \n",
      "    tokenizer=RegexpTokenizer(r'[a-zA-Z]+')\n",
      "    data_tokenized=[]\n",
      "    for data in text: \n",
      "        tokenized_text=tokenizer.tokenize(data.lower()) \n",
      "        data_tokenized.append([w for w in tokenized_text if len(w)>=3])\n",
      "    uniquefeatures=word_features(data_tokenized)\n",
      "\n",
      "    return data_tokenized,uniquefeatures   \n",
      "     \n",
      "def words_len3_remURL_andindents(text):\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "    tokenizer=RegexpTokenizer(r'[a-zA-Z]+')\n",
      "    data_tokenized=[]\n",
      "    for data in text: \n",
      "        data=URLless_string = re.sub(r'(?i)\\b((?:(http|https):[/|//]|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))', '', data)\n",
      "        data=re.sub(r'([a-zA-Z0-9][.|:])*','',data)\n",
      "        tokenized_text=tokenizer.tokenize(data.lower())\n",
      "        data_tokenized.append([w for w in tokenized_text if len(w)>=3])\n",
      "    uniquefeatures=word_features(data_tokenized)\n",
      "    return data_tokenized,uniquefeatures   \n",
      "        \n",
      "def wordsRemoveStopwords(text):\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "    other_stopwords=['app','android','github','com','http','twitter','author','pebble','gmail']\n",
      "    tokenizer=RegexpTokenizer(r'[a-zA-Z]+')\n",
      "    data_tokenized=[]\n",
      "    for data in text: \n",
      "        tokenized_text=tokenizer.tokenize(data.lower())\n",
      "        data_tokenized.append([w for w in tokenized_text if len(w)>=3])\n",
      "\n",
      "    for i in range(len(data_tokenized)):\n",
      "        data_tokenized[i] = [w for w in data_tokenized[i] if not w in [word.encode(\"utf-8\") for word in stopwords.words('english')]+other_stopwords]\n",
      "    uniquefeatures=word_features(data_tokenized)\n",
      "    return data_tokenized,uniquefeatures      \n",
      "    \n",
      "def wordsTranslateLen3(text):\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "        ##DIVIDE the phrase into parts and then translate\n",
      "    gs = goslate.Goslate()\n",
      "    #f=lambda x: gs.translate(x, 'en').encode(\"utf-8\")\n",
      "\n",
      "    for i in range(len(text)):\n",
      "        sentence_list=PunktSentenceTokenizer().tokenize(text[i].decode('latin-1'))\n",
      "        sentence_list=[w.encode(\"utf-8\")  for w in sentence_list if detect_language(w).encode(\"utf-8\") == 'english']\n",
      "            #sentence_list=[w.encode(\"utf-8\") for w in sentence_list]\n",
      "            \n",
      "            #sentence_list=map(f,sentence_list)\n",
      "        text[i]=[\"\".join(sentence_list)][0]\n",
      "           \n",
      "    (data_tokenized,uniquefeatures)=words_len3(text)\n",
      "    return data_tokenized,uniquefeatures\n",
      "\n",
      "        \n",
      "    return []\n",
      "    \n",
      "        \n",
      "def tfidf(text):\n",
      "    #from sklearn.feature_extraction import text\n",
      "    #my_stop_words = text.ENGLISH_STOP_WORDS\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "    other_stopwords=['app','android','github','com','http','twitter','author','pebble','gmail','app','android','github','com','http','twitter','author','pebble','gmail','watch','version','new','google','http','www']\n",
      "    stemmer=PorterStemmer()\n",
      "    text=tokenized_stemming(text)\n",
      "\n",
      "    #other_stopwords=['app','android','github','com','http','twitter','author','pebble','gmail','watch','version','new','google','http','www']\n",
      "    #stopWords = stopwords.words('english')+other_stopwords\n",
      "    vectorizer=TfidfVectorizer(decode_error=u'ignore',max_df=1.0, max_features=20000,\n",
      "                             stop_words=other_stopwords)\n",
      "    '''New thing'''\n",
      "    \n",
      "    \n",
      "    '''New Thing end'''\n",
      "    X_train = vectorizer.fit_transform(text)\n",
      "    X_train=X_train.toarray()\n",
      "    return X_train,vectorizer\n",
      "\n",
      "def verbs_noun(text):\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "    return []\n",
      "\n",
      "def words_stemming(text):\n",
      "    '''text should be input as a list of each element = description\n",
      "    This function returns the list of features of words'''\n",
      "    return []\n",
      "\n",
      "def word_features(tokenizedText):\n",
      "    '''Takes the tokenized list and returns unique features'''\n",
      "    aggregatedList=[]\n",
      "    for i in range(len(tokenizedText)):\n",
      "        aggregatedList=aggregatedList+tokenizedText[i]\n",
      "    uniquefeatures=list(set(aggregatedList))\n",
      "    return uniquefeatures\n",
      "\n",
      "def tokenized_stemming(text):\n",
      "    other_stopwords=['app','android','github','com','http','twitter','author','pebble','gmail','app','android','github','com','http','twitter','author','pebble','gmail','watch','version','new','google','http','www']\n",
      "    stemmer=PorterStemmer()\n",
      "    tokenizer=RegexpTokenizer(r'[a-zA-Z]+')\n",
      "    data_tokenized=[]\n",
      "    for data in text: \n",
      "        try:\n",
      "            tokenized_text=tokenizer.tokenize(str(data).lower())\n",
      "        except UnicodeEncodeError:\n",
      "            tokenized_text=tokenizer.tokenize('app')\n",
      "        data_tokenized.append([stemmer.stem(str(w)) for w in tokenized_text if len(w)>=3])\n",
      "    for i in range(len(data_tokenized)):\n",
      "        data_tokenized[i] = [str(w) for w in data_tokenized[i] if not w in [word.encode(\"utf-8\") for word in stopwords.words('english')]+other_stopwords]\n",
      "        data_tokenized[i]=[\" \".join(data_tokenized[i])][0]\n",
      "    return data_tokenized"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Matrix Extraction for description"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' This module creates Feature matrix either using Bag of Words/Tfidf etc.'''\n",
      "def create_featurematrix_bagofwords(data_tokenized,uniquefeatures):\n",
      "    '''This function is for creating a feature matrix based on word_features selected in the preprocessing step'''\n",
      "    ### CREATE featurematrix\n",
      "    X=np.array([])\n",
      "    for dataset in data_tokenized:\n",
      "        bagList=[]\n",
      "        for features in uniquefeatures:\n",
      "            bagList.append(dataset.count(features))\n",
      "        if X.shape[0]==0:\n",
      "            X=np.array(bagList)\n",
      "        else:\n",
      "            X=np.column_stack((X,np.array(bagList)))\n",
      "    X=X.T\n",
      "    ## CREATE TARGET\n",
      "    #totCategories=categories.unique\n",
      "    #for category in categories:        \n",
      "    #Y=categories\n",
      "    return X\n",
      " \n",
      "def create_featurematrix_onlyoccurence(data_tokenized,uniquefeatures):\n",
      "    '''This function is for creating a feature matrix based on word_features selected in the preprocessing step'''\n",
      "    ### CREATE featurematrix\n",
      "    X=np.array([])\n",
      "    for dataset in data_tokenized:\n",
      "        bagList=[]\n",
      "        for features in uniquefeatures:\n",
      "            if dataset.count(features)>0:\n",
      "                bagList.append(1)\n",
      "            else:\n",
      "                bagList.append(dataset.count(features))\n",
      "        if X.shape[0]==0:\n",
      "            X=np.array(bagList)\n",
      "        else:\n",
      "            X=np.column_stack((X,np.array(bagList)))\n",
      "    X=X.T\n",
      "    ## CREATE TARGET\n",
      "    #totCategories=categories.unique\n",
      "    #for category in categories:        \n",
      "    #Y=categories\n",
      "    return X\n",
      "\n",
      "def create_featurematrix_onlyPresetCat(Xcat):\n",
      "    '''Create Feature Matrix for Preset Categories'''\n",
      "    preset_unique=['Daily','Games','GetSomeApps','Health & Fitness','Index','Notifications','Remotes','Tools & Utilities']\n",
      "    df=DataFrame()\n",
      "    for l in preset_unique:\n",
      "        df[l]=np.zeros(len(Xcat))\n",
      "    for i in range(len(Xcat)):\n",
      "        df.ix[i,Xcat[i]]=1\n",
      "    ##\n",
      "    return np.array(df.ix[:,0:len(df.columns)])\n",
      "\n",
      "def create_featurematrix_alsoPresetCat_bagofwords(data_tokenized,uniquefeatures,Xcat):\n",
      "    '''This function is for creating a feature matrix based on word_features selected in the preprocessing step'''\n",
      "    X=create_featurematrix_bagofwords(data_tokenized,uniquefeatures)\n",
      "    X=np.hstack((X,create_featurematrix_onlyPresetCat(Xcat)))\n",
      "    return X\n",
      " \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Feature Reduction ??"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "''' PCA was used to see if Feature reduction can be done. In this case the Transformed variable did not cover a significant \n",
      "part of the variance. Collinearity was also checked for the features but did not help much either'''\n",
      "varK = [] # Variance for each reduced set\n",
      "for k in range(1,100,3000): # Vary the size of reduced set\n",
      "    pca=PCA(n_components=100)\n",
      "    pca.fit(X_train)\n",
      "    X_train=pca.transform(X_train)\n",
      "    varK(pca.explained_variance_ratio_)\n",
      "print varK"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Train and store ensemble models"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trainModel(X,Y,Xcat,preprocess,modelname,samPer,keyParam,model_accuracy,includePreset='n',interactions='n',noBstrap=100):\n",
      "    ''' Function to 1) Resample from data 2) train an ensemble of models\n",
      "    Input: X,Y : Data as numpy matrices\n",
      "    preprocessing step: Bag of words or TFIDF\n",
      "    modelname: LogisticRegression, MultinomialNN, or RandomForestClassifier\n",
      "    keyParam: a list of parameter to adjust based on bootstrap error, For eg: penalty for logistic regression\n",
      "    Model_accuracy: model_accuracy metric to use, precision/recall\n",
      "    includePreset: Should the preset categories be included in the training and prediction\n",
      "    interactions: Should the interactions be considered among the features\n",
      "    NoBstrap: Number of times to resample'''\n",
      "    \n",
      "    ## Ensemble of models\n",
      "    models=[]\n",
      "    \n",
      "    ## \" Test and Train split \"\n",
      "    for k in range(noBstrap): # No of times you want to resample\n",
      "        ## Generate the indices for train and test\n",
      "        train_index=sklearn.utils.resample(range(X.shape[0]))\n",
      "        test_index=[]\n",
      "        for i in range(X.shape[0]):\n",
      "            if i not in train_index:\n",
      "            test_index.append(i)\n",
      "        ## Split the data\n",
      "        description_train, description_test,Y_train,Y_test = X[train_index],X[test_index],Y[train_index],Y[test_index]\n",
      "    features=[]\n",
      "    avgtestacc=[]\n",
      "    avgtrainacc=[]\n",
      "    avgrandomacc=[]\n",
      "    \n",
      "        ## Train the models and choose the model that performs the best on the test set\n",
      "        for alp in keyParam:\n",
      "            trainacc=[]\n",
      "            testacc=[]\n",
      "            randacc=[]\n",
      "            model_alp\n",
      "            features_alp\n",
      "            \n",
      "            ### Define the model selected, \n",
      "            if modelname==LogisticRegression:\n",
      "                clf=modelname(C=alp)\n",
      "            if modelname==MultinomialNB:\n",
      "                clf = MultinomialNB(alpha=alp)\n",
      "            if modelname==RandomForestClassifier:\n",
      "                clf=RandomForestClassifier(n_estimators=alp)\n",
      "            \n",
      "            \n",
      "            ## CALL FEATURE CREATER AND FEATURE MATRIX CREATERS FOR TRAIN\n",
      "            if preprocess==tfidf:\n",
      "                [X_train,vectorizer]=tfidf(description_train)\n",
      "                features_alp.append(vectorizer)\n",
      "                X_test = vectorizer.transform(description_test)  ## Transformation of test should depend on the fit from train\n",
      "                X_test=X_test.toarray()               \n",
      "            else: \n",
      "                (data_tokenized,uniquefeatures)=preprocess(description_train)\n",
      "                ## CALL FEATURE MATRIX CREATER FOR TEST\n",
      "                X_train=create_featurematrix_bagofwords(data_tokenized,uniquefeatures)\n",
      "                (data_tokenized,uniquefeatures1)=preprocess(description_test)\n",
      "                X_test=create_featurematrix_bagofwords(data_tokenized,uniquefeatures)\n",
      "                features_alp.append(uniquefeatures)\n",
      "                \n",
      "            \n",
      "            ## Check if the user wants to consider Preset categories for prediction\n",
      "            if includePreset.lower()=='y':\n",
      "                X_train=np.hstack((X_train,create_featurematrix_onlyPresetCat(Xcat_train)))\n",
      "                X_test=np.hstack((X_test,create_featurematrix_onlyPresetCat(Xcat_test)))\n",
      "           \n",
      "           ## Check if interactions have to be included\n",
      "            nrows1=len(preset_unique)\n",
      "            if interactions.lower()=='y':\n",
      "                nrows=X_train.shape[0]\n",
      "                ncols=X_train.shape[1]\n",
      "                nrowstest=X_test.shape[0]\n",
      "                for int1 in range(ncols):\n",
      "                    for int2 in range(ncols):\n",
      "                        X_train=np.hstack((X_train,(X_train[:,int1]*X_train[:,int2]).reshape(nrows,1)))\n",
      "                        X_test=np.hstack((X_test,(X_test[:,int1]*X_test[:,int2]).reshape(nrowstest,1)))\n",
      "                \n",
      "            ## DO FITTING\n",
      "            clf.fit(X_train,Y_train)\n",
      "            model_alp.append(clf)\n",
      "            \n",
      "            ## COmpute accuracy for a test, train, and random error\n",
      "            Y_random=[]\n",
      "            for k in range(len(Y_test)):\n",
      "                Y_random.append(random.choice(list(Counter(list(Y)).keys())))\n",
      "            trainacc.append(model_accuracy(Y_train,clf.predict(X_train)))\n",
      "            testacc.append(model_accuracy(Y_test,clf.predict(X_test)))\n",
      "            randacc.append(model_accuracy(Y_test,Y_random))\n",
      "            \n",
      "            ### Choose the model with the  highest testacc\n",
      "            max_index=0\n",
      "            for k in range(len(testacc)):\n",
      "                if testacc[k]>testacc[max_index]:\n",
      "                    max_index=k\n",
      "            model.append[max_index]\n",
      "            features.append[max_index]\n",
      "            avgtestacc = testacc[max_index]\n",
      "            avgtrainacc = trainacc[max_index]\n",
      "            avgrandomacc = randacc[max_index]\n",
      "\n",
      "        #print 'test accuracy= ', testacc,'\\n','train accuracy= ',trainacc, '\\n','random accuracy= ',randacc\n",
      "        avgtestacc = avgtestacc/float(noBstrap)\n",
      "        avgtrainacc = avgtrainacc/float(noBstrap)\n",
      " \n",
      "        ### CHECK FOR WHICH SET OF PARAMETERS (key Param) the test error is the lowest and return that\n",
      "    print \"Average test accuracy = \",avgtestacc,\"Average train accuracy =\", avgtrainacc,\"Average random accuracy = \", avgrandomacc\n",
      "    return models,features,avgtestacc,avgtrainacc,avgrandomacc\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    }
   ],
   "metadata": {}
  }
 ]
}